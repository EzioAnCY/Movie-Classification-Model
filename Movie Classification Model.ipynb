{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.utils import to_categorical\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_excel(\"/Users/changyuean/Desktop/Quantiphi Case Study/Training sheet.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since id is not useful in the classification task, so I will delete id first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(['id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the 'total' column is highly correlated with Category and it doesn't exit in the test dataset, \n",
    "# so I will drop it.\n",
    "train_data['total'].corr(train_data['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(['total'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For movies names, it could have positive impact or negative impact or no impact on the category\n",
    "#For customers and investors, they know Harry Potter very well based on the previous movies or the book.\n",
    "#Another good example is Avatar. Investor cannot know what is Avatar just based on the name. \n",
    "#They need to look at the movie descriptions, movies lines, background stories etc to understand this movie.\n",
    "#As a result, movie name and display name do not have any impact in this case\n",
    "train_data = train_data.drop(['name'], axis =1)\n",
    "train_data = train_data.drop(['display_name'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see the distribution of the year\n",
    "sns.countplot(x='production_year',data=train_data,palette='hls')\n",
    "plt.show()\n",
    "plt.savefig('count_plot')\n",
    "#based on the result, the movies are well distributed by different years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see the distribution of creative type\n",
    "sns.countplot(x='creative_type',data=train_data,palette='hls')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "plt.savefig('count_plot')\n",
    "#most of the movies are contemporary fiction, but there are other movies in other creative types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see the distribution of the source\n",
    "sns.countplot(x='source',data=train_data,palette='hls')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "plt.savefig('count_plot')\n",
    "#most of the movies are \"original screenplay\", but there are movies from other sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see the distribution of the genre\n",
    "sns.countplot(x='genre',data=train_data,palette='hls')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "plt.savefig('count_plot')\n",
    "#based on the result, the movies genres are well distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see the distribution of the language\n",
    "sns.countplot(x='language',data=train_data,palette='hls')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "plt.savefig('count_plot')\n",
    "#based on the result, almost all movies are English. So language is not a good variable for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see the distribution of the product method\n",
    "sns.countplot(x='production_method',data=train_data,palette='hls')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "plt.savefig('count_plot')\n",
    "#based on the result, almost all movies are Live Action. I will keep this variable to see if it has significant result\n",
    "#or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see the unique values in the board_rating_reason\n",
    "train_data[\"board_rating_reason\"].unique()\n",
    "#there are so many unique values in this variable, and they are only descriptive words of the movie rating\n",
    "#so they are not very useful in this case, so we can delete it\n",
    "train_data = train_data.drop(['board_rating_reason'],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see the distribution of the movie board rating display name\n",
    "sns.countplot(x='movie_board_rating_display_name',data=train_data,palette='hls')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "plt.savefig('count_plot')\n",
    "#based on the result, the movies are well distributed to different ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see the distribution of the movie_release_pattern_display_name\n",
    "sns.countplot(x='movie_release_pattern_display_name',data=train_data,palette='hls')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "plt.savefig('count_plot')\n",
    "#based on the result, majority of the movies are Wide, but there are some movies in Limited, Expands Wide and Exclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see the distribution of the Category\n",
    "sns.countplot(x='Category',data=train_data,palette='hls')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "plt.savefig('count_plot')\n",
    "#based on the result, Category is well distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop language \n",
    "train_data = train_data.drop(['language'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer product year from int to categorical data\n",
    "train_data['production_year'] = train_data['production_year'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(train_data))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above results, we can see that there 1196 examples in the dataset and 10 variables. We need to transform the variables into dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dummy variable if it is categorical data and drop categorical columns\n",
    "def create_dummy(data_input):\n",
    "    var=None\n",
    "    column_name=list(data_input.columns)\n",
    "    for var in column_name:\n",
    "        if data_input[var].dtypes == 'object':\n",
    "            cat_list='var'+'_'+var\n",
    "            cat_list = pd.get_dummies(data_input[var], prefix=var)\n",
    "            data_input1=data_input.join(cat_list)\n",
    "            data_input=data_input1\n",
    "            data_input=data_input.drop([var],axis=1)\n",
    "    return data_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = create_dummy(train_data)\n",
    "print(np.shape(data_input))\n",
    "data_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the target variable \"Category\", we can see that it has multiple different values. As a result, this is a classification problem. In order to classify them, the first algorithm we can use is Softmax regression, aka multinomial logistic regression. For the Softmax regression, we can think it as a one layer, k neurons (in this case, k = 9) neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting to code the softmax regression algorithm, there is a very important data preparation step which is called \"one-hot coding\" for the targeted variable Category. The reason that I am doing the one-hot coding is the output of the softmax regression is a matrix in (1196,9) dimension. Each output associates with 9 predicted possibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(data):\n",
    "    \n",
    "    data_with_dummy= create_dummy(data)\n",
    "    dataY = data_with_dummy['Category']\n",
    "    dataX = data_with_dummy.drop(['Category'],axis = 1)\n",
    "    \n",
    "    data_one_hot_Y = to_categorical(dataY)\n",
    "    data_one_hot_Y = data_one_hot_Y[:,1:10] #drop the column 0\n",
    "    dataX = np.asarray(dataX)\n",
    "    \n",
    "    return dataX, data_one_hot_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX, data_one_hot_Y = onehot(train_data)\n",
    "print(data_one_hot_Y.shape)\n",
    "print(dataX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing one-hot coding, then we split the data into train (72%), dev (18%), and test set (10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the data from Pandas Frame to numpy array and split into Train, Dev and Test Set\n",
    "def data_preprocess(data):\n",
    "    \n",
    "    dataX, data_one_hot_Y= onehot(data)\n",
    "    #split train, dev, and test datasets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(dataX, data_one_hot_Y, test_size=0.1, random_state=0)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_split(k,x_train,y_train):\n",
    "    kf = KFold(n_splits=k, random_state=0)\n",
    "    all_kfold_trainX = []\n",
    "    all_kfold_testX = []\n",
    "    all_kfold_trainY = []\n",
    "    all_kfold_testY = []\n",
    "    for train_index, test_index in kf.split(x_train):\n",
    "        kfold_trainX, kfold_testX = x_train[train_index], x_train[test_index]\n",
    "        kfold_trainY, kfold_testY = y_train[train_index], y_train[test_index]\n",
    "        all_kfold_trainX.append(kfold_trainX)\n",
    "        all_kfold_testX.append(kfold_testX)\n",
    "        all_kfold_trainY.append(kfold_trainY)\n",
    "        all_kfold_testY.append(kfold_testY)\n",
    "    \n",
    "    return all_kfold_trainX, all_kfold_testX, all_kfold_trainY, all_kfold_testY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the data, we start to build the Softmax regression algorithm.\n",
    "For the (i)th specific node, the prediction Å· = exp(z[i]) / Î£(exp(z[i]), i=1,2,3,.....,m and z[i] = w[i]*x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x_train, W):\n",
    "\n",
    "    Z = np.dot(x_train, W)\n",
    "\n",
    "    soft_max = np.exp(Z)/ np.sum(np.exp(Z),axis=1, keepdims=True)\n",
    "    \n",
    "    return soft_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will use the cross-entropy loss to calculate the loss function of the prediction: \n",
    "L = -Î£ylog(Å·) with L2 regularization. Also, we will calculate gradient descent of W. After calculating the loss function, we will use gradient descent to update the weights W and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(x, y, lambd, lr, W):\n",
    "    soft_max = softmax(x,W)\n",
    "    m = x.shape[0]\n",
    "    loss = (-1/m)*np.sum(y*np.log(soft_max)) + (lambd/2)*np.sum(W*W) #calculate the cross entropy loss\n",
    "    grad = (-1/m)*np.dot(x.T, y - soft_max) + lambd*W #calculate the gradient descent\n",
    "    W = W - lr*grad #using gradient descent to update W\n",
    "    return loss, grad, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply softmax model on the test dataset for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(x_test, y_test, W):\n",
    "    m = x_test.shape[0]\n",
    "    soft_max = softmax(x_test, W)\n",
    "    soft_max_maximum = np.max(soft_max,axis=1)\n",
    "    soft_max_maximum = soft_max_maximum.reshape(soft_max_maximum.shape[0],1)\n",
    "    pred = ((soft_max-soft_max_maximum)==0)\n",
    "    result = np.zeros(x_test.shape[0])\n",
    "    for i in range(x_test.shape[0]):\n",
    "        result[i] = (np.sum(pred[i]==y_test[i])==9)\n",
    "    \n",
    "    Accuracy = np.sum(result)/m\n",
    "    return Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the main function. In the main function, we will decide the threshold to make sure the gradient descent is going to converge. The threshold is 1e-5. Otherwise, the maximum number of iterations will be 3000. Learning rate will 0.2. Lambda will be 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_softmax(data, k, maxiterations=3000, learning_rate=0.2, lambd=0.1):\n",
    "\n",
    "    x_train, x_test, y_train, y_test = data_preprocess(data)\n",
    "    \n",
    "    all_kfold_trainX, all_kfold_testX, all_kfold_trainY, all_kfold_testY = kfold_split(k,x_train,y_train)\n",
    "    \n",
    "    all_accuracy = []\n",
    "    all_weights = []\n",
    "    \n",
    "    for i in range(k):     \n",
    "        W = np.random.randn(all_kfold_trainX[i].shape[1],9)\n",
    "        soft_max = softmax(all_kfold_trainX[i], W)\n",
    "        loss, grad, W = cross_entropy(all_kfold_trainX[i], all_kfold_trainY[i], lambd, learning_rate, W)\n",
    "        losses = []\n",
    "        losses.append(loss)\n",
    "        threshold = 1e-6\n",
    "        prev_loss = 0\n",
    "        j=1\n",
    "        while abs(prev_loss - loss)>threshold and j < maxiterations:\n",
    "            soft_max = softmax(all_kfold_trainX[i], W)\n",
    "            prev_loss = loss\n",
    "            loss, grad, W = cross_entropy(all_kfold_trainX[i], all_kfold_trainY[i], lambd, learning_rate, W)\n",
    "            j+=1\n",
    "            \n",
    "            \"\"\"\n",
    "            # Print the cost every 100 training example\n",
    "            if j % 100 == 0:\n",
    "                print (\"Loss after iteration %i: %f\" %(j, loss))\n",
    "            if j % 100 == 0:\n",
    "                losses.append(loss)\n",
    "            \"\"\"\n",
    "               \n",
    "        Accuracy = prediction(all_kfold_testX[i], all_kfold_testY[i], W)\n",
    "        print(\"Accuracy is :\", Accuracy, \"in fold \", i)\n",
    "        all_weights.append(W)\n",
    "        all_accuracy.append(Accuracy)\n",
    "    \n",
    "    print(\"The mean accuracy of\", k, \"fold validation is: \", np.mean(all_accuracy))\n",
    "    \n",
    "    mean_weight = np.mean(all_weights, axis = 0)\n",
    "    \n",
    "    Accuracy = prediction(x_test, y_test,  mean_weight)\n",
    "    print(\"Test Set Accuracy is :\", Accuracy)\n",
    "    \n",
    "    return mean_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = main_softmax(train_data, k=10, maxiterations=8000, learning_rate=0.005, lambd=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Softmax regression I got 29.16% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is another way of doing classification. The best of Random Forest is it is an extension over bagging. It takes one extra step where in addition to taking the random subset of data, it also takes the random selection of features rather than using all features to grow trees. According to the Bias-Variance Analysis: Variance(x) = ðœŒ(ðœŽ^2) + (1-ðœŒ)/M * (ðœŽ^2). So as we increase the number of sample trees M, the Variance will reduce. Their ability to limit overfitting without substantially increasing error due to bias is why I am going to use this model for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = a.shape[0]\n",
    "count=0\n",
    "for i in range(m):\n",
    "    if a[i]==9:\n",
    "        count+=1\n",
    "\n",
    "accuracy = count/m\n",
    "\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomforest(dev_trainX, dev_trainY, nestimators, maxdepth, minsampleleaf, minsamplesplit):\n",
    "    \n",
    "    #start to create the Random Forest Model\n",
    "    rf_Model= RandomForestClassifier(n_estimators=nestimators, max_depth=maxdepth, min_samples_leaf= minsampleleaf,\n",
    "                                 min_samples_split = minsamplesplit, random_state=0)\n",
    "    \n",
    "    rf_Model.fit(dev_trainX, dev_trainY)\n",
    "    #print(rf_Model.feature_importances_)\n",
    "    \n",
    "    \n",
    "    return rf_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_RF(data, k, nestimators, maxdepth, minsampleleaf, minsamplesplit):\n",
    "\n",
    "    data_with_dummy = create_dummy(data)\n",
    "    \n",
    "    Y = data_with_dummy['Category']\n",
    "    X = data_with_dummy.drop(['Category'], axis = 1)\n",
    "    \n",
    "    Y = np.asarray(Y)\n",
    "    X = np.asarray(X)\n",
    "    \n",
    "    #just split original data, but not doing one-hot encoding\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
    "    \n",
    "    \n",
    "    all_kfold_trainX, all_kfold_testX, all_kfold_trainY, all_kfold_testY = kfold_split(k,x_train,y_train)\n",
    "    \n",
    "    best_accuracy = 0 #save the best accuracy\n",
    "    best_hyperparameter = np.zeros(4) #to save the best combination of 4 different hyper-parameters\n",
    "    best_model = None\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in range(k):\n",
    "        for n in nestimators:\n",
    "            for ma in maxdepth:\n",
    "                for msl in minsampleleaf:\n",
    "                    for mss in minsamplesplit:\n",
    "                        m = all_kfold_testX[i].shape[0]\n",
    "                        result = np.zeros(m)\n",
    "                        rf_Model = randomforest(all_kfold_trainX[i], all_kfold_trainY[i], n, ma, msl, mss)\n",
    "                        pred_result = rf_Model.predict(all_kfold_testX[i])\n",
    "                        \n",
    "                        #start to calculate the accuracy\n",
    "                        for j in range(all_kfold_testX[i].shape[0]):\n",
    "                            result[j] = (pred_result[j]==all_kfold_trainY[i][j])\n",
    "                            \n",
    "                        Accuracy = np.sum(result)/m\n",
    "                        \n",
    "                        #to figure out the best combination of the hyper-parameters\n",
    "                        if Accuracy> best_accuracy:\n",
    "                            best_accuracy = Accuracy\n",
    "                            best_hyperparameter = [n, ma, msl, mss]\n",
    "                            best_model = rf_Model\n",
    "                            \n",
    "                        print(\"Current progress of hyper-parameter tuning: \", n, ma, msl, mss, Accuracy)\n",
    "                            \n",
    "    print(\"Best\",k,\"fold accuracy is: \", best_accuracy)\n",
    "    print(\"Best n-estimators: \", best_hyperparameter[0])\n",
    "    print(\"Best max_depth: \", best_hyperparameter[1])\n",
    "    print(\"Best min_sample_leaf: \", best_hyperparameter[2])\n",
    "    print(\"Best min_sample_split: \", best_hyperparameter[3])\n",
    "    \n",
    "    #using test dataset for prediction\n",
    "    pred_result = best_model.predict(x_test)\n",
    "    \n",
    "    m = x_test.shape[0]\n",
    "    result = np.zeros(m)\n",
    "    #calcuate the accuracy\n",
    "    for i in range(m):\n",
    "        result[i] = pred_result[i]==y_test[i]\n",
    "    \n",
    "    Accuracy = np.sum(result)/x_test.shape[0]\n",
    "    \n",
    "    print(\"The Test Dataset Accuracy is: \",Accuracy)\n",
    "    \n",
    "    return best_hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameter = main_RF(train_data, k=10, nestimators=random.sample(range(200,500), 2), maxdepth = random.sample(range(1,50), 2), \n",
    "                              minsampleleaf = random.sample(range(5,20), 2), minsamplesplit = random.sample(range(5,30), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
